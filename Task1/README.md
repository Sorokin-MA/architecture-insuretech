# Задание 1

## Цель

Обеспечить высокую отказоустойчивость и одинаковое время отклика для пользователей в регионах РФ. Выполнить RTO — 45 мин., RPO — 15 мин., достигнуть доступности 99,9%.

## Ключевые решения

- Мультирегиональная архитектура с активным сервисом в двух регионах
- Независимые Kubernetes-кластеры в каждом регионе (без растянутого кластера)
- Глобальный балансировщик с DNS и мониторингом здоровья регионов (низкий TTL)
- CDN для статического контента и кэширования чтения на периметре
- Внутренний балансировщик уровня 7 (Ingress), health check по /health
- Управляемый PostgreSQL кластер с синхронной репликацией в зоне и асинхронной между регионами
- Локальные read replica и Redis для ускорения чтений и снижения нагрузки

## Топология решения

Приложение работает в двух регионах с двумя зонами доступности на каждый. В каждом регионе свой Kubernetes-кластер. Трафик пользователя направляется через глобальный DNS к ближайшему региону. Статика через CDN, динамические запросы — на региональный кластер. Проверка здоровья сервисов осуществляется постоянно.

## Kubernetes

Выбраны независимые кластеры для устранения рисков split-brain и уменьшения задержек. Включён autoscaling (HPA) по CPU/памяти, настроены лимиты и disruption budget. Применяется распределение по зонам с topology spread.

## Балансировка и heath checks

Глобальный DNS с TTL 20 секунд и мониторингом состояния. Региональный балансировщик уровня 7 с проверкой /health каждой службы. Для партнерского API введены лимиты запросов, circuit breaker и таймауты. Веб-клиенты защищены WAF.

## Стратегия фейловера

При потере зоны в регионе трафик перераспределяется внутри региона. При отказе региона глобальный DNS переводит трафик на другой регион. Восстановление сервисов происходит в пределах 45 минут благодаря IaC и автоматизации. Потеря данных не превышает 15 минут. Используется Active-Standby с георезервированием.

## База данных

Переход на управляемый кластер PostgreSQL с мастером и синхронным стендбаем в основном регионе и асинхронной репликой в резервном регионе. Хранение WAL для восстановления до точки времени. Ежедневные бэкапы с 30-дневным хранением. В приложении реализованы повторные попытки и идемпотентность.
Объём 50 Гб не требует шардирования. Для истории заявок предусмотрено секционирование по времени для оптимизации запросов.

## Равномерное время отклика

Статика обслуживается CDN. Чтение данных происходит из локальных реплик и кеша Redis. Все кэшируемые ответы имеют ограниченный TTL. Записи обрабатываются в основном регионе с использованием материализованных представлений для ускорения.

## Наблюдаемость

Сбор метрик Prometheus, централизованная трассировка и логирование. Настроены алерты по доступности, ошибкам и задержкам с оповещением команды поддержки.

## Итог

Решение обеспечивает заданные RTO, RPO и доступность через мультизонное мультирегиональное развертывание. Оптимизировано под равномерное время отклика пользователей за счет географического распределения и кэширования. Стоимость и сложность контролируются с помощью управляемых сервисов и автоматизации.
